<h2>Preparation of the training data frame</h2>

<p>Fist of of all we load the data</p>

<pre><code class="r">training_base = read.csv(&quot;../data/pml-training.csv&quot;)
testing_base  = read.csv(&quot;../data/pml-testing.csv&quot;)
</code></pre>

<p>Variables refered to bell, belt, dum, arm and fore arm are selected</p>

<pre><code class="r">all_var = names(training_base)
var_selected = grep(&quot;bell|belt|dum|arm&quot;, all_var)
</code></pre>

<p>Other variables are eliminated</p>

<pre><code class="r">train_numeric = training_base[,var_selected]
</code></pre>

<p>Since there are a lot of variables containg NA values, we 
substitute non-numeric by NA and eliminating columns containing NA</p>

<pre><code class="r">train_numeric = lapply(train_numeric,as.character)
train_numeric = lapply(train_numeric,as.numeric)
train_numeric = as.data.frame(train_numeric)
training = cbind(training_base$classe,train_numeric) 
training = training[,colSums(is.na(training)) == 0]
</code></pre>

<p>Finally we add the classes to the data frame</p>

<pre><code class="r">names(training)[1] = paste(&quot;classe&quot;)
</code></pre>

<h2>Creation of the model</h2>

<p>Now we create a model to predict the &ldquo;classe&rdquo;</p>

<p>We load the library &ldquo;caret&rdquo;</p>

<pre><code class="r">require(caret)
</code></pre>

<p>We create a partition of available training data. Since we have a lot of 
values and this is a prototype we choose a small probability of being selected</p>

<pre><code class="r">prob =0.1
set.seed=123
inTrain = createDataPartition(y=training$classe, p=prob,list=F)
train_train = training[inTrain,]
train_test = training[-inTrain,]
</code></pre>

<p>Now we create the model</p>

<pre><code class="r">model = train(classe~.,data=train_train,method=&quot;rf&quot;)
</code></pre>

<pre><code>## Loading required package: randomForest
## randomForest 4.6-10
## Type rfNews() to see new features/changes/bug fixes.
</code></pre>

<h2>Application of the model to the data not included in the model</h2>

<p>We now apply to the remaining data</p>

<pre><code class="r">cm = confusionMatrix(train_test$classe,predict(model,train_test))
print(cm)
</code></pre>

<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 4928   51   17   26    0
##          B  201 3087  114   12    3
##          C    2   95 2925   54    3
##          D   42   36  140 2674    2
##          E   12   87   42   60 3045
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9434          
##                  95% CI : (0.9399, 0.9468)
##     No Information Rate : 0.2936          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.9284          
##  Mcnemar&#39;s Test P-Value : &lt; 2.2e-16       
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9504   0.9198   0.9033   0.9462   0.9974
## Specificity            0.9925   0.9769   0.9893   0.9852   0.9862
## Pos Pred Value         0.9813   0.9034   0.9500   0.9240   0.9381
## Neg Pred Value         0.9797   0.9811   0.9785   0.9897   0.9994
## Prevalence             0.2936   0.1901   0.1834   0.1600   0.1729
## Detection Rate         0.2791   0.1748   0.1656   0.1514   0.1724
## Detection Prevalence   0.2844   0.1935   0.1744   0.1639   0.1838
## Balanced Accuracy      0.9714   0.9484   0.9463   0.9657   0.9918
</code></pre>

<p>We can see that the accuracy is higher than .94</p>

<h2>Application to the testing data</h2>

<pre><code class="r">predict(model, testing_base)
</code></pre>

<pre><code>##  [1] B A B A A E D B A A B C B A E E A B B B
## Levels: A B C D E
</code></pre>

